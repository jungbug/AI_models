{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.190 ðŸš€ Python-3.8.16 torch-2.0.1 CPU (Apple M2 Pro)\n",
      "Setup complete âœ… (12 CPUs, 16.0 GB RAM, 361.5/460.4 GB disk)\n"
     ]
    }
   ],
   "source": [
    "import ultralytics\n",
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt to 'yolov8n.pt'...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.23M/6.23M [00:01<00:00, 3.67MB/s]\n",
      "Ultralytics YOLOv8.0.190 ðŸš€ Python-3.8.16 torch-2.0.1 CPU (Apple M2 Pro)\n",
      "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n",
      "\n",
      "Downloading https://ultralytics.com/images/zidane.jpg to 'zidane.jpg'...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 165k/165k [00:00<00:00, 1.03MB/s]\n",
      "image 1/1 /Volumes/ë¬´ì œ/coding/AI/yolo v8/zidane.jpg: 384x640 2 persons, 1 tie, 43.4ms\n",
      "Speed: 3.8ms preprocess, 43.4ms inference, 5.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
      "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/predict\n"
     ]
    }
   ],
   "source": [
    "!yolo predict model=yolov8n.pt source='https://ultralytics.com/images/zidane.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 475M/780M [01:35<01:02, 5.15MB/s] "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.hub.download_url_to_file('https://ultralytics.com/assets/coco2017val.zip', 'tmp.zip')  # download (780M - 5000 images)\n",
    "!unzip -q tmp.zip -d datasets && rm tmp.zip  # unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.190 ðŸš€ Python-3.8.16 torch-2.0.1 CPU (Apple M2 Pro)\n",
      "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n",
      "\n",
      "Dataset 'coco8.yaml' images not found âš ï¸, missing path '/Volumes/ë¬´ì œ/coding/AI/yolo v8/datasets/coco8/images/val'\n",
      "Downloading https://ultralytics.com/assets/coco8.zip to '/Volumes/ë¬´ì œ/coding/AI/yolo v8/datasets/coco8.zip'...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 433k/433k [00:00<00:00, 2.06MB/s]\n",
      "Unzipping /Volumes/ë¬´ì œ/coding/AI/yolo v8/datasets/coco8.zip to /Volumes/ë¬´ì œ/co\n",
      "Dataset download success âœ… (1.5s), saved to \u001b[1m/Volumes/ë¬´ì œ/coding/AI/yolo v8/datasets\u001b[0m\n",
      "\n",
      "Fontconfig warning: ignoring UTF-8: not a valid region tag\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Volumes/ë¬´ì œ/coding/AI/yolo v8/datasets/coco8/labels/val... 4 ima\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Volumes/ë¬´ì œ/coding/AI/yolo v8/datasets/coco8/labels/val.cache\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all          4         17      0.621      0.833      0.888       0.63\n",
      "                person          4         10      0.721        0.5      0.519      0.269\n",
      "                   dog          4          1       0.37          1      0.995      0.597\n",
      "                 horse          4          2      0.751          1      0.995      0.631\n",
      "              elephant          4          2      0.505        0.5      0.828      0.394\n",
      "              umbrella          4          1      0.564          1      0.995      0.995\n",
      "          potted plant          4          1      0.814          1      0.995      0.895\n",
      "Speed: 1.1ms preprocess, 62.2ms inference, 0.0ms loss, 0.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val\u001b[0m\n",
      "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/val\n"
     ]
    }
   ],
   "source": [
    "!yolo val model=yolov8n.pt data=coco8.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.190 ðŸš€ Python-3.8.16 torch-2.0.1 CPU (Apple M2 Pro)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=coco8.yaml, epochs=3, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train\n",
      "Fontconfig warning: ignoring UTF-8: not a valid region tag\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    897664  ultralytics.nn.modules.head.Detect           [80, [64, 128, 256]]          \n",
      "Model summary: 225 layers, 3157200 parameters, 3157184 gradients, 8.9 GFLOPs\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Volumes/ë¬´ì œ/coding/AI/yolo v8/datasets/coco8/labels/train... 4\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /Volumes/ë¬´ì œ/coding/AI/yolo v8/datasets/coco8/labels/train.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Volumes/ë¬´ì œ/coding/AI/yolo v8/datasets/coco8/labels/val.cache...\u001b[0m\n",
      "Plotting labels to runs/detect/train/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000119, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train\u001b[0m\n",
      "Starting training for 3 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        1/3         0G     0.9792      3.465      1.356         32        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all          4         17      0.902      0.518      0.727      0.494\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        2/3         0G      1.363      2.846      1.662         22        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all          4         17      0.906      0.524      0.738      0.515\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        3/3         0G      1.294      3.584      1.698         17        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all          4         17      0.907      0.525      0.729       0.52\n",
      "\n",
      "3 epochs completed in 0.001 hours.\n",
      "Optimizer stripped from runs/detect/train/weights/last.pt, 6.5MB\n",
      "Optimizer stripped from runs/detect/train/weights/best.pt, 6.5MB\n",
      "\n",
      "Validating runs/detect/train/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.190 ðŸš€ Python-3.8.16 torch-2.0.1 CPU (Apple M2 Pro)\n",
      "Model summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all          4         17      0.907      0.525      0.743      0.527\n",
      "                person          4         10       0.94        0.3      0.502      0.236\n",
      "                   dog          4          1          1          0      0.332      0.189\n",
      "                 horse          4          2          1      0.852      0.995      0.747\n",
      "              elephant          4          2          1          0      0.638        0.2\n",
      "              umbrella          4          1      0.759          1      0.995      0.895\n",
      "          potted plant          4          1      0.745          1      0.995      0.895\n",
      "Speed: 1.2ms preprocess, 55.9ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train\u001b[0m\n",
      "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/train\n"
     ]
    }
   ],
   "source": [
    "# Train YOLOv8n on COCO8 for 3 epochs\n",
    "!yolo train model=yolov8n.pt data=coco8.yaml epochs=3 imgsz=640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.190 ðŸš€ Python-3.8.16 torch-2.0.1 CPU (Apple M2 Pro)\n",
      "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolov8n.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (6.2 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTorchScript:\u001b[0m starting export with torch 2.0.1...\n",
      "\u001b[34m\u001b[1mTorchScript:\u001b[0m export success âœ… 0.8s, saved as 'yolov8n.torchscript' (12.4 MB)\n",
      "\n",
      "Export complete (2.1s)\n",
      "Results saved to \u001b[1m/Volumes/ë¬´ì œ/coding/AI/yolo v8\u001b[0m\n",
      "Predict:         yolo predict task=detect model=yolov8n.torchscript imgsz=640  \n",
      "Validate:        yolo val task=detect model=yolov8n.torchscript imgsz=640 data=coco.yaml  \n",
      "Visualize:       https://netron.app\n",
      "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/export\n"
     ]
    }
   ],
   "source": [
    "!yolo export model=yolov8n.pt format=torchscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    897664  ultralytics.nn.modules.head.Detect           [80, [64, 128, 256]]          \n",
      "YOLOv8n summary: 225 layers, 3157200 parameters, 3157184 gradients, 8.9 GFLOPs\n",
      "\n",
      "Ultralytics YOLOv8.0.190 ðŸš€ Python-3.8.16 torch-2.0.1 CPU (Apple M2 Pro)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=coco128.yaml, epochs=3, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train4\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    897664  ultralytics.nn.modules.head.Detect           [80, [64, 128, 256]]          \n",
      "Model summary: 225 layers, 3157200 parameters, 3157184 gradients, 8.9 GFLOPs\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train4', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Volumes/ë¬´ì œ/coding/AI/yolo v8/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Volumes/ë¬´ì œ/coding/AI/yolo v8/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [00:00<?, ?it/s]\n",
      "Plotting labels to runs/detect/train4/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000119, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train4\u001b[0m\n",
      "Starting training for 3 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        1/3         0G      1.096      1.402      1.205        267        640:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:50<00:07,  7.17s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Volumes/ë¬´ì œ/coding/AI/yolo v8/yolo.ipynb Cell 7\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Volumes/%EB%AC%B4%EC%A0%9C/coding/AI/yolo%20v8/yolo.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model \u001b[39m=\u001b[39m YOLO(\u001b[39m'\u001b[39m\u001b[39myolov8n.pt\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# load a pretrained model (recommended for training)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Volumes/%EB%AC%B4%EC%A0%9C/coding/AI/yolo%20v8/yolo.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Use the model\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Volumes/%EB%AC%B4%EC%A0%9C/coding/AI/yolo%20v8/yolo.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m results \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain(data\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcoco128.yaml\u001b[39;49m\u001b[39m'\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)  \u001b[39m# train the model\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Volumes/%EB%AC%B4%EC%A0%9C/coding/AI/yolo%20v8/yolo.ipynb#W6sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m results \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mval()  \u001b[39m# evaluate model performance on the validation set\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Volumes/%EB%AC%B4%EC%A0%9C/coding/AI/yolo%20v8/yolo.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m results \u001b[39m=\u001b[39m model(\u001b[39m'\u001b[39m\u001b[39mhttps://ultralytics.com/images/bus.jpg\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# predict on an image\u001b[39;00m\n",
      "File \u001b[0;32m/Volumes/ë¬´ì œ/coding/AI/yolo v8/ultralytics/ultralytics/engine/model.py:334\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mmodel\n\u001b[1;32m    333\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mhub_session \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession  \u001b[39m# attach optional HUB session\u001b[39;00m\n\u001b[0;32m--> 334\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m    335\u001b[0m \u001b[39m# Update model and cfg after training\u001b[39;00m\n\u001b[1;32m    336\u001b[0m \u001b[39mif\u001b[39;00m RANK \u001b[39min\u001b[39;00m (\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m):\n",
      "File \u001b[0;32m/Volumes/ë¬´ì œ/coding/AI/yolo v8/ultralytics/ultralytics/engine/trainer.py:195\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    192\u001b[0m         ddp_cleanup(\u001b[39mself\u001b[39m, \u001b[39mstr\u001b[39m(file))\n\u001b[1;32m    194\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 195\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_train(world_size)\n",
      "File \u001b[0;32m/Volumes/ë¬´ì œ/coding/AI/yolo v8/ultralytics/ultralytics/engine/trainer.py:355\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtloss \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtloss \u001b[39m*\u001b[39m i \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_items) \u001b[39m/\u001b[39m (i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtloss \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \\\n\u001b[1;32m    352\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_items\n\u001b[1;32m    354\u001b[0m \u001b[39m# Backward\u001b[39;00m\n\u001b[0;32m--> 355\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscaler\u001b[39m.\u001b[39;49mscale(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss)\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    357\u001b[0m \u001b[39m# Optimize - https://pytorch.org/docs/master/notes/amp_examples.html\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[39mif\u001b[39;00m ni \u001b[39m-\u001b[39m last_opt_step \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccumulate:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tensorflow_macos/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tensorflow_macos/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO('yolov8n.yaml')  # build a new model from scratch\n",
    "model = YOLO('yolov8n.pt')  # load a pretrained model (recommended for training)\n",
    "\n",
    "# Use the model\n",
    "results = model.train(data='coco128.yaml', epochs=3)  # train the model\n",
    "results = model.val()  # evaluate model performance on the validation set\n",
    "results = model('https://ultralytics.com/images/bus.jpg')  # predict on an image\n",
    "results = model.export(format='onnx')  # export the model to ONNX format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.190 ðŸš€ Python-3.8.16 torch-2.0.1 CPU (Apple M2 Pro)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=coco128.yaml, epochs=3, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    897664  ultralytics.nn.modules.head.Detect           [80, [64, 128, 256]]          \n",
      "Model summary: 225 layers, 3157200 parameters, 3157184 gradients, 8.9 GFLOPs\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train3', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Volumes/ë¬´ì œ/coding/AI/yolo v8/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Volumes/ë¬´ì œ/coding/AI/yolo v8/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [00:00<?, ?it/s]\n",
      "Plotting labels to runs/detect/train3/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000119, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train3\u001b[0m\n",
      "Starting training for 3 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        1/3         0G      1.096      1.365      1.202        201        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:47<00:00,  5.90s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:20<00:00,  5.05s/it]\n",
      "                   all        128        929      0.647      0.518      0.594      0.441\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        2/3         0G      1.216      1.443      1.268        136        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:46<00:00,  5.77s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:20<00:00,  5.03s/it]\n",
      "                   all        128        929      0.666      0.537      0.617      0.458\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        3/3         0G      1.192      1.342      1.242        206        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:47<00:00,  6.00s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:20<00:00,  5.06s/it]\n",
      "                   all        128        929      0.668      0.551      0.626      0.466\n",
      "\n",
      "3 epochs completed in 0.057 hours.\n",
      "Optimizer stripped from runs/detect/train3/weights/last.pt, 6.5MB\n",
      "Optimizer stripped from runs/detect/train3/weights/best.pt, 6.5MB\n",
      "\n",
      "Validating runs/detect/train3/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.190 ðŸš€ Python-3.8.16 torch-2.0.1 CPU (Apple M2 Pro)\n",
      "Model summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:18<00:00,  4.69s/it]\n",
      "                   all        128        929       0.67      0.552      0.625      0.466\n",
      "                person        128        254      0.791       0.67      0.762      0.546\n",
      "               bicycle        128          6      0.606      0.333      0.323       0.28\n",
      "                   car        128         46      0.693      0.217      0.276      0.171\n",
      "            motorcycle        128          5       0.68      0.858      0.938      0.764\n",
      "              airplane        128          6      0.731      0.667       0.85      0.598\n",
      "                   bus        128          7      0.534      0.658      0.657      0.595\n",
      "                 train        128          3      0.538      0.667      0.806      0.752\n",
      "                 truck        128         12          1      0.375      0.499      0.304\n",
      "                  boat        128          6      0.298      0.167      0.386      0.237\n",
      "         traffic light        128         14      0.686      0.214      0.206       0.14\n",
      "             stop sign        128          2          1      0.945      0.995      0.703\n",
      "                 bench        128          9      0.784      0.408      0.623      0.385\n",
      "                  bird        128         16      0.918      0.699      0.902      0.545\n",
      "                   cat        128          4       0.86          1      0.995       0.82\n",
      "                   dog        128          9      0.727      0.886      0.869      0.634\n",
      "                 horse        128          2      0.569          1      0.995      0.597\n",
      "              elephant        128         17      0.958      0.824      0.897      0.701\n",
      "                  bear        128          1      0.639          1      0.995      0.895\n",
      "                 zebra        128          4       0.86          1      0.995      0.965\n",
      "               giraffe        128          9      0.748      0.889      0.961       0.72\n",
      "              backpack        128          6       0.59      0.333      0.394      0.264\n",
      "              umbrella        128         18      0.735        0.5      0.684      0.454\n",
      "               handbag        128         19      0.472     0.0497      0.157     0.0899\n",
      "                   tie        128          7       0.83      0.699      0.702      0.492\n",
      "              suitcase        128          4      0.553          1      0.895      0.621\n",
      "               frisbee        128          5      0.598        0.8      0.733       0.64\n",
      "                  skis        128          1       0.45          1      0.497      0.211\n",
      "             snowboard        128          7      0.778      0.714      0.762      0.488\n",
      "           sports ball        128          6      0.738      0.476      0.573      0.322\n",
      "                  kite        128         10      0.817       0.45      0.558      0.173\n",
      "          baseball bat        128          4      0.482        0.5      0.534      0.228\n",
      "        baseball glove        128          7      0.653      0.429      0.429      0.309\n",
      "            skateboard        128          5      0.781        0.6        0.6      0.426\n",
      "         tennis racket        128          7      0.607      0.286      0.426      0.305\n",
      "                bottle        128         18      0.476      0.354        0.4      0.234\n",
      "            wine glass        128         16      0.625      0.375      0.572      0.341\n",
      "                   cup        128         36       0.74      0.278      0.432      0.297\n",
      "                  fork        128          6          1      0.298      0.374      0.241\n",
      "                 knife        128         16      0.611      0.562      0.594      0.344\n",
      "                 spoon        128         22       0.69      0.182      0.369      0.202\n",
      "                  bowl        128         28       0.65      0.643      0.634      0.526\n",
      "                banana        128          1     0.0679      0.339      0.199     0.0494\n",
      "              sandwich        128          2       0.39      0.669      0.745      0.745\n",
      "                orange        128          4          1      0.421      0.995       0.67\n",
      "              broccoli        128         11       0.52      0.182      0.257      0.229\n",
      "                carrot        128         24      0.621      0.458      0.663      0.412\n",
      "               hot dog        128          2      0.555          1      0.828      0.795\n",
      "                 pizza        128          5       0.66          1      0.995      0.866\n",
      "                 donut        128         14      0.614          1      0.899      0.823\n",
      "                  cake        128          4      0.775          1      0.995      0.834\n",
      "                 chair        128         35      0.471      0.514      0.436      0.264\n",
      "                 couch        128          6      0.741      0.483      0.665      0.528\n",
      "          potted plant        128         14      0.562      0.571      0.678      0.472\n",
      "                   bed        128          3       0.97      0.667       0.83      0.668\n",
      "          dining table        128         13      0.612      0.608      0.531      0.432\n",
      "                toilet        128          2       0.64        0.5      0.828      0.796\n",
      "                    tv        128          2      0.535        0.5      0.662      0.629\n",
      "                laptop        128          3          1          0      0.577      0.451\n",
      "                 mouse        128          2          1          0          0          0\n",
      "                remote        128          8      0.766        0.5      0.573       0.48\n",
      "            cell phone        128          8          0          0     0.0418     0.0208\n",
      "             microwave        128          3      0.494          1      0.863      0.736\n",
      "                  oven        128          5      0.364        0.4      0.365      0.286\n",
      "                  sink        128          6      0.351      0.167      0.251      0.169\n",
      "          refrigerator        128          5      0.542        0.4      0.616       0.49\n",
      "                  book        128         29      0.624      0.116      0.273      0.142\n",
      "                 clock        128          9       0.78      0.789      0.897      0.738\n",
      "                  vase        128          2      0.429          1      0.828      0.795\n",
      "              scissors        128          1          1          0      0.249     0.0746\n",
      "            teddy bear        128         21      0.976      0.381      0.618      0.417\n",
      "            toothbrush        128          5          1      0.538        0.8      0.511\n",
      "Speed: 0.7ms preprocess, 142.2ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train3\u001b[0m\n",
      "\n",
      "Found https://ultralytics.com/images/bus.jpg locally at bus.jpg\n",
      "image 1/1 /Volumes/ë¬´ì œ/coding/AI/yolo v8/bus.jpg: 640x480 4 persons, 1 bus, 1 stop sign, 47.8ms\n",
      "Speed: 1.7ms preprocess, 47.8ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: None\n",
       " names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
       " orig_img: array([[[122, 148, 172],\n",
       "         [120, 146, 170],\n",
       "         [125, 153, 177],\n",
       "         ...,\n",
       "         [157, 170, 184],\n",
       "         [158, 171, 185],\n",
       "         [158, 171, 185]],\n",
       " \n",
       "        [[127, 153, 177],\n",
       "         [124, 150, 174],\n",
       "         [127, 155, 179],\n",
       "         ...,\n",
       "         [158, 171, 185],\n",
       "         [159, 172, 186],\n",
       "         [159, 172, 186]],\n",
       " \n",
       "        [[128, 154, 178],\n",
       "         [126, 152, 176],\n",
       "         [126, 154, 178],\n",
       "         ...,\n",
       "         [158, 171, 185],\n",
       "         [158, 171, 185],\n",
       "         [158, 171, 185]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[185, 185, 191],\n",
       "         [182, 182, 188],\n",
       "         [179, 179, 185],\n",
       "         ...,\n",
       "         [114, 107, 112],\n",
       "         [115, 105, 111],\n",
       "         [116, 106, 112]],\n",
       " \n",
       "        [[157, 157, 163],\n",
       "         [180, 180, 186],\n",
       "         [185, 186, 190],\n",
       "         ...,\n",
       "         [107,  97, 103],\n",
       "         [102,  92,  98],\n",
       "         [108,  98, 104]],\n",
       " \n",
       "        [[112, 112, 118],\n",
       "         [160, 160, 166],\n",
       "         [169, 170, 174],\n",
       "         ...,\n",
       "         [ 99,  89,  95],\n",
       "         [ 96,  86,  92],\n",
       "         [102,  92,  98]]], dtype=uint8)\n",
       " orig_shape: (1080, 810)\n",
       " path: '/Volumes/ë¬´ì œ/coding/AI/yolo v8/bus.jpg'\n",
       " probs: None\n",
       " save_dir: None\n",
       " speed: {'preprocess': 1.7271041870117188, 'inference': 47.76430130004883, 'postprocess': 0.5891323089599609}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load YOLOv8n, train it on COCO128 for 3 epochs and predict an image with it\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('yolov8n.pt')  # load a pretrained YOLOv8n detection model\n",
    "model.train(data='coco128.yaml', epochs=3)  # train the model\n",
    "model('https://ultralytics.com/images/bus.jpg')  # predict on an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLOv8n-seg, train it on COCO128-seg for 3 epochs and predict an image with it\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('yolov8n-seg.pt')  # load a pretrained YOLOv8n segmentation model\n",
    "model.train(data='coco128-seg.yaml', epochs=3)  # train the model\n",
    "model('https://ultralytics.com/images/bus.jpg')  # predict on an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLOv8n-cls, train it on mnist160 for 3 epochs and predict an image with it\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('yolov8n-cls.pt')  # load a pretrained YOLOv8n classification model\n",
    "model.train(data='mnist160', epochs=3)  # train the model\n",
    "model('https://ultralytics.com/images/bus.jpg')  # predict on an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLOv8n-pose, train it on COCO8-pose for 3 epochs and predict an image with it\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('yolov8n-pose.pt')  # load a pretrained YOLOv8n classification model\n",
    "model.train(data='coco8-pose.yaml', epochs=3)  # train the model\n",
    "model('https://ultralytics.com/images/bus.jpg')  # predict on an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ultralytics'ì— ë³µì œí•©ë‹ˆë‹¤...\n",
      "remote: Enumerating objects: 16335, done.\u001b[K\n",
      "remote: Counting objects: 100% (50/50), done.\u001b[K\n",
      "remote: Compressing objects: 100% (47/47), done.\u001b[K\n",
      "remote: Total 16335 (delta 20), reused 11 (delta 3), pack-reused 16285\u001b[K\n",
      "ì˜¤ë¸Œì íŠ¸ë¥¼ ë°›ëŠ” ì¤‘: 100% (16335/16335), 8.66 MiB | 5.75 MiB/s, ì™„ë£Œ.\n",
      "ë¸íƒ€ë¥¼ ì•Œì•„ë‚´ëŠ” ì¤‘: 100% (11330/11330), ì™„ë£Œ.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ultralytics/ultralytics -b main\n",
    "%pip install -qe ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: pytest\n"
     ]
    }
   ],
   "source": [
    "# Run tests (Git clone only)\n",
    "!pytest ultralytics/tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.190 ðŸš€ Python-3.8.16 torch-2.0.1 CPU (Apple M2 Pro)\n",
      "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n",
      "Fontconfig warning: ignoring UTF-8: not a valid region tag\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Volumes/ë¬´ì œ/coding/AI/yolo v8/datasets/coco/labels/val2017... 49\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Volumes/ë¬´ì œ/coding/AI/yolo v8/datasets/coco/labels/val2017.cache\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       5000      36335      0.633      0.475      0.521      0.371\n",
      "                person       5000      10777      0.754      0.673      0.745      0.514\n",
      "               bicycle       5000        314      0.687      0.392      0.457      0.265\n",
      "                   car       5000       1918      0.646      0.515      0.561      0.364\n",
      "            motorcycle       5000        367       0.71       0.58      0.655      0.414\n",
      "              airplane       5000        143      0.814      0.766      0.832      0.653\n",
      "                   bus       5000        283      0.746      0.643      0.739       0.62\n",
      "                 train       5000        190      0.798       0.77      0.834      0.646\n",
      "                 truck       5000        414      0.549      0.399      0.435      0.293\n",
      "                  boat       5000        424      0.583        0.3      0.377       0.21\n",
      "         traffic light       5000        634      0.644      0.345      0.409      0.211\n",
      "          fire hydrant       5000        101       0.85      0.703      0.774      0.609\n",
      "             stop sign       5000         75      0.695       0.64      0.692       0.63\n",
      "         parking meter       5000         60      0.631        0.5      0.558      0.441\n",
      "                 bench       5000        411       0.57      0.258      0.296      0.193\n",
      "                  bird       5000        427      0.686      0.358      0.425      0.278\n",
      "                   cat       5000        202      0.776      0.824      0.856      0.652\n",
      "                   dog       5000        218      0.656      0.701      0.729      0.591\n",
      "                 horse       5000        272        0.7      0.653      0.693      0.525\n",
      "                 sheep       5000        354      0.609      0.667      0.662       0.46\n",
      "                   cow       5000        372      0.697      0.601      0.682      0.487\n",
      "              elephant       5000        252      0.702      0.833      0.821      0.629\n",
      "                  bear       5000         71      0.847      0.775      0.842      0.689\n",
      "                 zebra       5000        266      0.808      0.797      0.882      0.659\n",
      "               giraffe       5000        232      0.857      0.828      0.885      0.683\n",
      "              backpack       5000        371      0.489      0.156      0.197        0.1\n",
      "              umbrella       5000        407      0.631      0.504      0.542      0.359\n",
      "               handbag       5000        540      0.479      0.126      0.169     0.0848\n",
      "                   tie       5000        252      0.686      0.361      0.429      0.268\n",
      "              suitcase       5000        299      0.566      0.431      0.502      0.342\n",
      "               frisbee       5000        115      0.748      0.774      0.767      0.584\n",
      "                  skis       5000        241      0.609      0.323      0.366      0.188\n",
      "             snowboard       5000         69      0.483      0.319      0.388      0.266\n",
      "           sports ball       5000        260      0.713      0.438      0.474      0.328\n",
      "                  kite       5000        327      0.605       0.52      0.558       0.38\n",
      "          baseball bat       5000        145      0.585      0.386      0.403      0.216\n",
      "        baseball glove       5000        148      0.669      0.473      0.512      0.302\n",
      "            skateboard       5000        179      0.721      0.608      0.663      0.451\n",
      "             surfboard       5000        267      0.624      0.473      0.507      0.309\n",
      "         tennis racket       5000        225      0.702      0.618      0.671      0.397\n",
      "                bottle       5000       1013      0.612      0.387      0.454      0.298\n",
      "            wine glass       5000        341      0.659      0.352       0.42       0.27\n",
      "                   cup       5000        895      0.584      0.443      0.488       0.35\n",
      "                  fork       5000        215      0.583      0.318      0.389      0.263\n",
      "                 knife       5000        325      0.505      0.163      0.174      0.106\n",
      "                 spoon       5000        253       0.41      0.146       0.16     0.0988\n",
      "                  bowl       5000        623      0.589      0.494      0.529       0.39\n",
      "                banana       5000        370      0.549      0.319      0.374      0.233\n",
      "                 apple       5000        236      0.414      0.242      0.227      0.156\n",
      "              sandwich       5000        177       0.57      0.486      0.463      0.349\n",
      "                orange       5000        285      0.439      0.418      0.369      0.281\n",
      "              broccoli       5000        312      0.476       0.34      0.368      0.209\n",
      "                carrot       5000        365      0.446      0.293      0.307      0.189\n",
      "               hot dog       5000        125       0.76      0.432      0.497      0.364\n",
      "                 pizza       5000        284      0.658      0.613      0.658      0.503\n",
      "                 donut       5000        328      0.575      0.494      0.515      0.408\n",
      "                  cake       5000        310      0.533       0.39      0.437      0.292\n",
      "                 chair       5000       1771      0.581       0.34      0.403      0.257\n",
      "                 couch       5000        261      0.593      0.552      0.585      0.434\n",
      "          potted plant       5000        342      0.521      0.386      0.385      0.225\n",
      "                   bed       5000        163       0.55      0.552      0.589      0.427\n",
      "          dining table       5000        695      0.529       0.44      0.435      0.293\n",
      "                toilet       5000        179      0.721      0.737      0.775      0.642\n",
      "                    tv       5000        288      0.771      0.635      0.715      0.553\n",
      "                laptop       5000        231      0.682      0.662      0.699       0.58\n",
      "                 mouse       5000        106      0.634      0.651      0.708      0.527\n",
      "                remote       5000        283       0.43      0.223       0.27       0.16\n",
      "              keyboard       5000        153      0.595      0.604       0.65      0.482\n",
      "            cell phone       5000        262      0.548      0.351        0.4       0.28\n",
      "             microwave       5000         55      0.616      0.545      0.643      0.514\n",
      "                  oven       5000        143      0.643      0.448      0.511      0.349\n",
      "               toaster       5000          9      0.726      0.222      0.436      0.314\n",
      "                  sink       5000        225      0.568      0.444      0.508      0.337\n",
      "          refrigerator       5000        126      0.662      0.587       0.65      0.511\n",
      "                  book       5000       1129      0.489      0.111      0.197     0.0969\n",
      "                 clock       5000        267       0.75      0.607      0.667      0.457\n",
      "                  vase       5000        274      0.588      0.453      0.453      0.321\n",
      "              scissors       5000         36      0.689      0.333      0.332      0.278\n",
      "            teddy bear       5000        190      0.669      0.558      0.604       0.42\n",
      "            hair drier       5000         11          1          0    0.00492    0.00377\n",
      "            toothbrush       5000         57      0.402      0.193      0.236      0.164\n",
      "Speed: 0.3ms preprocess, 153.0ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "Saving runs/detect/val3/predictions.json...\n",
      "\n",
      "Evaluating pycocotools mAP using runs/detect/val3/predictions.json and /Volumes/ë¬´ì œ/coding/AI/yolo v8/datasets/coco/annotations/instances_val2017.json...\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['pycocotools>=2.0.6'] not found, attempting AutoUpdate...\n",
      "Collecting pycocotools>=2.0.6\n",
      "  Downloading pycocotools-2.0.7-cp38-cp38-macosx_10_9_universal2.whl (168 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m168.8/168.8 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/jeongseong-yun/opt/anaconda3/envs/tensorflow_macos/lib/python3.8/site-packages (from pycocotools>=2.0.6) (1.24.3)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /Users/jeongseong-yun/opt/anaconda3/envs/tensorflow_macos/lib/python3.8/site-packages (from pycocotools>=2.0.6) (3.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/jeongseong-yun/opt/anaconda3/envs/tensorflow_macos/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.6) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/jeongseong-yun/opt/anaconda3/envs/tensorflow_macos/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.6) (9.4.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/jeongseong-yun/opt/anaconda3/envs/tensorflow_macos/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.6) (4.25.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/jeongseong-yun/opt/anaconda3/envs/tensorflow_macos/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.6) (5.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/jeongseong-yun/opt/anaconda3/envs/tensorflow_macos/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.6) (1.4.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/jeongseong-yun/opt/anaconda3/envs/tensorflow_macos/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.6) (1.0.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/jeongseong-yun/opt/anaconda3/envs/tensorflow_macos/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.6) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/jeongseong-yun/opt/anaconda3/envs/tensorflow_macos/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.6) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jeongseong-yun/opt/anaconda3/envs/tensorflow_macos/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.6) (23.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/jeongseong-yun/opt/anaconda3/envs/tensorflow_macos/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib>=2.1.0->pycocotools>=2.0.6) (3.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jeongseong-yun/opt/anaconda3/envs/tensorflow_macos/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>=2.0.6) (1.16.0)\n",
      "Installing collected packages: pycocotools\n",
      "Successfully installed pycocotools-2.0.7\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success âœ… 2.1s, installed 1 package: ['pycocotools>=2.0.6']\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m âš ï¸ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n",
      "loading annotations into memory...\n",
      "Done (t=0.17s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=1.06s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=23.17s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=6.46s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.374\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.526\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.405\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.186\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.410\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.535\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.320\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.533\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.589\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.369\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.654\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.768\n",
      "Results saved to \u001b[1mruns/detect/val3\u001b[0m\n",
      "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/val\n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s.pt to 'yolov8s.pt'...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21.5M/21.5M [00:06<00:00, 3.75MB/s]\n",
      "Ultralytics YOLOv8.0.190 ðŸš€ Python-3.8.16 torch-2.0.1 CPU (Apple M2 Pro)\n",
      "YOLOv8s summary (fused): 168 layers, 11156544 parameters, 0 gradients, 28.6 GFLOPs\n",
      "Fontconfig warning: ignoring UTF-8: not a valid region tag\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Volumes/ë¬´ì œ/coding/AI/yolo v8/datasets/coco/labels/val2017.cache\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m^C\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jeongseong-yun/opt/anaconda3/envs/tensorflow_macos/bin/yolo\", line 33, in <module>\n",
      "    sys.exit(load_entry_point('ultralytics', 'console_scripts', 'yolo')())\n",
      "  File \"/Volumes/ë¬´ì œ/coding/AI/yolo v8/ultralytics/ultralytics/cfg/__init__.py\", line 445, in entrypoint\n",
      "    getattr(model, mode)(**overrides)  # default args from model\n",
      "  File \"/Volumes/ë¬´ì œ/coding/AI/yolo v8/ultralytics/ultralytics/engine/model.py\", line 269, in val\n",
      "    validator(model=self.model)\n",
      "  File \"/Users/jeongseong-yun/opt/anaconda3/envs/tensorflow_macos/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Volumes/ë¬´ì œ/coding/AI/yolo v8/ultralytics/ultralytics/engine/validator.py\", line 169, in __call__\n",
      "    preds = model(batch['img'], augment=augment)\n",
      "  File \"/Users/jeongseong-yun/opt/anaconda3/envs/tensorflow_macos/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Volumes/ë¬´ì œ/coding/AI/yolo v8/ultralytics/ultralytics/nn/autobackend.py\", line 339, in forward\n",
      "    y = self.model(im, augment=augment, visualize=visualize) if augment or visualize else self.model(im)\n",
      "  File \"/Users/jeongseong-yun/opt/anaconda3/envs/tensorflow_macos/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Volumes/ë¬´ì œ/coding/AI/yolo v8/ultralytics/ultralytics/nn/tasks.py\", line 45, in forward\n",
      "    return self.predict(x, *args, **kwargs)\n",
      "  File \"/Volumes/ë¬´ì œ/coding/AI/yolo v8/ultralytics/ultralytics/nn/tasks.py\", line 62, in predict\n",
      "    return self._predict_once(x, profile, visualize)\n",
      "  File \"/Volumes/ë¬´ì œ/coding/AI/yolo v8/ultralytics/ultralytics/nn/tasks.py\", line 82, in _predict_once\n",
      "    x = m(x)  # run\n",
      "  File \"/Users/jeongseong-yun/opt/anaconda3/envs/tensorflow_macos/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Volumes/ë¬´ì œ/coding/AI/yolo v8/ultralytics/ultralytics/nn/modules/block.py\", line 182, in forward\n",
      "    return self.cv2(torch.cat(y, 1))\n",
      "  File \"/Users/jeongseong-yun/opt/anaconda3/envs/tensorflow_macos/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Volumes/ë¬´ì œ/coding/AI/yolo v8/ultralytics/ultralytics/nn/modules/conv.py\", line 42, in forward_fuse\n",
      "    return self.act(self.conv(x))\n",
      "  File \"/Users/jeongseong-yun/opt/anaconda3/envs/tensorflow_macos/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jeongseong-yun/opt/anaconda3/envs/tensorflow_macos/lib/python3.8/site-packages/torch/nn/modules/conv.py\", line 463, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "  File \"/Users/jeongseong-yun/opt/anaconda3/envs/tensorflow_macos/lib/python3.8/site-packages/torch/nn/modules/conv.py\", line 459, in _conv_forward\n",
      "    return F.conv2d(input, weight, bias, self.stride,\n",
      "KeyboardInterrupt\n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8m.pt to 'yolov8m.pt'...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49.7M/49.7M [00:10<00:00, 4.98MB/s]\n",
      "Ultralytics YOLOv8.0.190 ðŸš€ Python-3.8.16 torch-2.0.1 CPU (Apple M2 Pro)\n",
      "YOLOv8m summary (fused): 218 layers, 25886080 parameters, 0 gradients, 78.9 GFLOPs\n",
      "Fontconfig warning: ignoring UTF-8: not a valid region tag\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Volumes/ë¬´ì œ/coding/AI/yolo v8/datasets/coco/labels/val2017.cache\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m^C\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jeongseong-yun/opt/anaconda3/envs/tensorflow_macos/bin/yolo\", line 33, in <module>\n",
      "    sys.exit(load_entry_point('ultralytics', 'console_scripts', 'yolo')())\n",
      "  File \"/Volumes/ë¬´ì œ/coding/AI/yolo v8/ultralytics/ultralytics/cfg/__init__.py\", line 445, in entrypoint\n",
      "    getattr(model, mode)(**overrides)  # default args from model\n",
      "  File \"/Volumes/ë¬´ì œ/coding/AI/yolo v8/ultralytics/ultralytics/engine/model.py\", line 269, in val\n",
      "    validator(model=self.model)\n",
      "  File \"/Users/jeongseong-yun/opt/anaconda3/envs/tensorflow_macos/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Volumes/ë¬´ì œ/coding/AI/yolo v8/ultralytics/ultralytics/engine/validator.py\", line 169, in __call__\n",
      "    preds = model(batch['img'], augment=augment)\n",
      "  File \"/Users/jeongseong-yun/opt/anaconda3/envs/tensorflow_macos/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Volumes/ë¬´ì œ/coding/AI/yolo v8/ultralytics/ultralytics/nn/autobackend.py\", line 339, in forward\n",
      "    y = self.model(im, augment=augment, visualize=visualize) if augment or visualize else self.model(im)\n",
      "  File \"/Users/jeongseong-yun/opt/anaconda3/envs/tensorflow_macos/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Volumes/ë¬´ì œ/coding/AI/yolo v8/ultralytics/ultralytics/nn/tasks.py\", line 45, in forward\n",
      "    return self.predict(x, *args, **kwargs)\n",
      "  File \"/Volumes/ë¬´ì œ/coding/AI/yolo v8/ultralytics/ultralytics/nn/tasks.py\", line 62, in predict\n",
      "    return self._predict_once(x, profile, visualize)\n",
      "  File \"/Volumes/ë¬´ì œ/coding/AI/yolo v8/ultralytics/ultralytics/nn/tasks.py\", line 82, in _predict_once\n",
      "    x = m(x)  # run\n",
      "  File \"/Users/jeongseong-yun/opt/anaconda3/envs/tensorflow_macos/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Volumes/ë¬´ì œ/coding/AI/yolo v8/ultralytics/ultralytics/nn/modules/block.py\", line 182, in forward\n",
      "    return self.cv2(torch.cat(y, 1))\n",
      "  File \"/Users/jeongseong-yun/opt/anaconda3/envs/tensorflow_macos/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Volumes/ë¬´ì œ/coding/AI/yolo v8/ultralytics/ultralytics/nn/modules/conv.py\", line 42, in forward_fuse\n",
      "    return self.act(self.conv(x))\n",
      "  File \"/Users/jeongseong-yun/opt/anaconda3/envs/tensorflow_macos/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/jeongseong-yun/opt/anaconda3/envs/tensorflow_macos/lib/python3.8/site-packages/torch/nn/modules/conv.py\", line 463, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "  File \"/Users/jeongseong-yun/opt/anaconda3/envs/tensorflow_macos/lib/python3.8/site-packages/torch/nn/modules/conv.py\", line 459, in _conv_forward\n",
      "    return F.conv2d(input, weight, bias, self.stride,\n",
      "KeyboardInterrupt\n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8l.pt to 'yolov8l.pt'...\n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 38.4M/83.7M [00:11<00:14, 3.26MB/s]"
     ]
    }
   ],
   "source": [
    "# Validate multiple models\n",
    "for x in 'nsmlx':\n",
    "  !yolo val model=yolov8{x}.pt data=coco.yaml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
